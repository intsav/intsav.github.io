
<!DOCTYPE html>
<html lang="en">

<!-- ======= Head Section ======= -->

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title>Foreshortening</title>
  <!-- IntSaV Logo-->
  <link href="assets/img/projects/Biobank/Foreshortening/Illustration.png" rel="icon">
  <!-- Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i"
    rel="stylesheet">
  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">
</head>

<!-- ======= Body Section ======= -->

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top">
    <div class="container d-flex align-items-center justify-content-between">
      <a href="index.html" class="logo"><img src="assets/img/projects/Biobank/logo-bio.png" alt="" class="img-fluid"></a>
      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto" href="">Home</a></li>
          <li><a class="nav-link scrollto" href="#projects">Projects</a></li>
          <li><a class="nav-link scrollto" href="#resources">Resources</a></li>
          <li><a class="nav-link scrollto" href="#funding">Funding</a></li>
          <li><a class="nav-link scrollto" href="#partners">Partners</a></li>
          <li><a class="nav-link scrollto" href="#footer">Contact</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav>
    </div>
  </header><!-- End Header -->
  <!-- ======= Title Section ======= -->
  <section id="project-title" class="d-flex align-items-center">
    <div class="container position-relative" data-aos="fade-up" data-aos-delay="100">
      <div class="row">
        <!--<div class="col-12 text-left"> -->
          <h1> A4C Foreshortening Assessment Project</h1>
        </div>
      </div>
  </section><!-- End Title -->


    <!-- Main Webpage Section -->
  <main id="project-main">

    <!-- ======= Project Body Section ======= -->
    <section id="project-body">

      <!-- ======= Project Introduction ======= -->
      <div class="project-intro">
        <div class="container">
          <div class="row justify-content-left">
            <div class="col-12">
              <div align="justify">
              <hr>
              <h1><b>Project Description</b> </h1>
              <br>
              <h3> Foreshortening is a common challenge in acquiring high-quality apical four-chamber (A4C) echocardiographic images. It occurs when the apex of the heart is not adequately visualised, leading to inaccurate assessments of cardiac structure and function. This project aims to create a standardised, open-access dataset of A4C images with varying degrees of foreshortening, along with expert annotations.
              </h3>
            </div>
            </div>
          </div>

          </div>
        </div>
      
      
              <h2>Project Goals</h2>
<ul>
    <li><i class="bi bi-check-circle"></i> **Develop a Reference Dataset:** Create a comprehensive dataset of A4C echocardiographic images with diverse foreshortening levels, carefully annotated by experienced echocardiographers.</li>
    <li><i class="bi bi-check-circle"></i> <b>Standardize Foreshortening Assessment:</b> Establish clear and objective criteria for assessing the degree of foreshortening in A4C images.</li>
    <li><i class="bi bi-check-circle"></i> <b>Empower AI Development:</b> Provide a high-quality dataset to train and validate AI algorithms for automatic foreshortening detection and correction.</li>
    <li><i class="bi bi-check-circle"></i> <b>Improve Image Acquisition:</b> Develop training materials and guidelines to help sonographers optimize image acquisition and minimize foreshortening.</li>
    <li><i class="bi bi-check-circle"></i> <b>Enhance Diagnostic Accuracy:</b> Ultimately improve the accuracy and reliability of A4C echocardiographic interpretations for better patient care.</li>
</ul>
      
      
      
      
      
      
      
      
      
      
      
      

      <!-- ======= Project Dataset ======= -->
      <div class = "project-dataset">
        <div class="container project-container">
          <div class="row justify-content-left">
            <div class="col-12 col-md-12 col-lg-6 col-xl-6" >
              <div align="center">
                <br><br><br>
            <img class="img-fluid" src="assets\img\projects\lv_segmentation\dataset_sample.PNG" alt="IMAGE Dataset">
            <h3> Fig. 1. An example 2D 4-chamber view. The blue and yellow curves represent the annotations
              by Operator-A and Operator-B, respectively</h3>
            </div>
          </div>
            <div class="col-12 col-md-12 col-lg-6 col-xl-6">
              <div align="justify">
              <h2>Project Goals</h2>
<ul>
    <li><i class="bi bi-check-circle"></i> **Develop a Reference Dataset:** Create a comprehensive dataset of A4C echocardiographic images with diverse foreshortening levels, carefully annotated by experienced echocardiographers.</li>
    <li><i class="bi bi-check-circle"></i> **Standardize Foreshortening Assessment:** Establish clear and objective criteria for assessing the degree of foreshortening in A4C images.</li>
    <li><i class="bi bi-check-circle"></i> **Empower AI Development:** Provide a high-quality dataset to train and validate AI algorithms for automatic foreshortening detection and correction.</li>
    <li><i class="bi bi-check-circle"></i> **Improve Image Acquisition:** Develop training materials and guidelines to help sonographers optimize image acquisition and minimize foreshortening.</li>
    <li><i class="bi bi-check-circle"></i> **Enhance Diagnostic Accuracy:** Ultimately improve the accuracy and reliability of A4C echocardiographic interpretations for better patient care.</li>
</ul>
                
                
              <br>
            </div>
            </div>
          </div>
        </div>
      </div>


        <!-- ======= Project Architecture ======= -->
      <div class = "project-architecture">

          <div class="container">
            <hr>
            <div class="row justify">
              <div class="col-12">
              <h2><b>Network Architecture</b></h2>
            </div>
              <div class="col-12 col-md-12 col-lg-6 col-xl-6">
                <div align="justify">
                <h3> <br><P>Standard and well-established U-Net neural network architecture was firstly used
                  since this architecture is applicable to multiple medical image segmentation problems.
                  The U-Net architecture comprises of three main steps such as down-sampling, upsampling steps and cross-over connections. During the down-sampling stage, the number
                  of features will increase gradually while during up-sampling stage the original image
                  resolution will recover. Also, cross-over connection is used by concatenating equally
                  size feature maps from down-sampling to the up-sampling to recover features that may
                  be lost during the down-sampling process.<br><br>
                  Each down-sampling and up-sampling has five levels, and each level has two convolutional layers with the same number of kernels ranging from 64 to 1024 from top to bottom
                  correspondingly. All convolutions kernels have a size of (3 × 3). For down-sampling
                  Max pooling with size (2 × 2) and equal strides was used.
                  In addition to the U-net, SegNet and FC-DenseNet models were also investigated.
                  The SegNet model contains an encoder stage, a corresponding decoder stage followed
                  by a pixel-wise classification layer. In SegNet model, to accomplish non-linear upsampling, the decoder performs pooling indices computed in the max-pooling step of
                  the corresponding encoder. The number of kernels and kernel size was the same as
                  the U-Net model.<br><br>
                  FC-DenseNet model is a relatively more recent model which consists of a downsampling and up-sampling path made of dense block. The down-sampling path is composed of two Transitions Down (TD) while an up-sampling path is containing two
                  Transitions Up (TU). Before and after each dense block, there is concatenation and
                  skip connections (see Fig. 2). The connectivity pattern in the up-sampling is different
                  from the down-sampling path. In the down-sampling path, the input to a dense block is
                  concatenated with its output, leading to linear growth of the number of feature maps,
                  whereas in the up-sampling path, it is not.
                  All models produce the output with the same spatial size as the input image
                  (i.e., 320 × 240).


                  </h3>
                </div>
              </div>
                <div class="col-12 col-md-12 col-lg-6 col-xl-6  text-center ">
                  <img class="img-fluid" src="assets\img\projects\lv_segmentation\Arch.PNG" alt="Architecture">
                  <div align="center">
                  <h3> Fig. 2. Diagram of FC-DenseNet architecture for semantic segmentation </h3>
                </div>
                </div>
              </div>
            </div>
          </div>

          <!-- ======= Project Implementation ======= -->
          <div class = "project-implementation">
            <div class="container">
              <div class="row justify-content-left">
                <div class="col-12">
                  <div align="justify">
                  <hr>
                  <h2><b>Implementation:</b> </h2>
                  <h3><p class="text-justify">Pytorch was used for the implementations [10], where Adam optimiser with 250 epochs and learning rate of 0.00001 were used for training the models.
                  The network weights are initialised randomly but differ in range depending on the size
                  of the previous layer. <br>Negative log-likelihood loss is used as the network’s objective
                  function. All computations were carried using an Nvidia GeForce GTX 1080 Ti GPU.
                  All models were trained separately and indecently using the annotations provided
                  by either of the operators, and following acronyms are used for the sake of simplicity:
                  GTOA and TOB as ground-truth segmentations provided by Operator-A and Operator-B,
                  respectively; POA and POB as Predicted LV borders by deep learning models trained
                  using GTOA and TOB.
                  </h3>
                  <div class="text-left">
                    <br>
                  <!--  <a class="btn btn-primary" href="" role="button">Download the code</a> -->
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      <!-- ======= Project Evaluation ======= -->
      <div class = "project-evaluation">
        <div class="container">
          <div class="row justify-content-left">
            <div class="col-12">
              <div align="justify">

              <hr>
              <div class="text-justify">
                <h2><b>Evaluation Measures</b></h2>
                <h3>The Dice Coefficient (DC), Hausdorff distance (HD), and intersection-over-union (IoU)
                  also known as the Jaccard index were employed to evaluate the performance and accuracy
                  of the CNN models in segmenting the LV region. The DC  was calculated to measure
                  the overlapping regions of the Predicted segmentation (P) and the ground truth (GT).
                  The range of DC is a value between 0 and 1, which 0 indicates there is not any overlap
                  between two sets of binary segmentation results while 1, indicates complete overlap.<br><br>
                  Also, the HD was calculated using the following formula for the contour of segmentation where, d(j, GT, P) is the distance from contour point j in GT to the closest contour
                  point in P. The number of pixels on the contour of GT and P specified with O and M
                  respectively.<br>
                  Moreover, the IoU was calculated image-by-image between the Predicted segmentation (IP) and the ground truth (GT). For a binary image (one foreground class, one
                background class), IoU is defined for the ground truth and predicted segmentation GT
                and IP
              </h3>
              </div>
              </div>
            </div>
            </div>
          </div>
        </div>

      <!-- ======= Project Results ======= -->

      <div class = "project-results">

          <div class="container">
            <hr>
            <div class="row justify">
              <div class="col-12">
            <h2><b>Experiment Results and Discussion </b><br></h2>
            </div>
              <div class="col-12 col-md-12 col-lg-6 col-xl-6">
                <div align="justify">
                <h3><br><br> <P> <br>Figure 3 shows example outputs from the three models when trained using annotation
                 provided by Operator-A (i.e., GTOA). The contour of the predicted segmentation was
                 used to specify the LV endocardium border. The red, solid line represents the automated
                 results, while the green line represents the manual annotation.<br>
                 <br>As can be seen, the U-Net model achieved higher DC (0.98), higher IoU (0.99), and
                 lower HD (4.24) score. A visual inspection of the automatically detected LV border also
                 confirms this. The LV border obtained from the SegNet and FC-DenseNet models seems
                 to be less smooth compared to that in the U-Net model. However, all three models seem
                 to perform with reasonable accuracy.<br> <br><br><br>
               <p> Figure 4 illustrates the results for a sample failed case, for which all three models
                 seem to struggle with the task of LV segmentation. By closer scrutiny of the echo images
                 for such cases, it is evident that the image quality tends to be lower due to missing borders,presence of speckle noise or artefacts,
                 and poor contrast between the myocardium and
                 the blood pool.
                 <br></h3>

                </div>
              </div>
                <div class="col-12 col-md-12 col-lg-6 col-xl-6  text-center ">
                  <br>
                  <img class="img-fluid" src="assets\img\projects\lv_segmentation\result1-1.PNG" alt="results">
                  <img class="img-fluid" src="assets\img\projects\lv_segmentation\result2-1.PNG" alt="results">
                </div>
              </div>
            </div>
          </div>

          <div class="container">
            <div class="row align-middle">
              <div class="col-12 col-lg-7 col-md-7 text-center">
                <img class="img-fluid" src="assets\img\projects\lv_segmentation\table1.PNG" alt="table1" >
                <h3>Table 1. Comparison of evaluation measures of (DC), (HD), and (IoU) between the three examine models</h3>
                <img class="img-fluid" src="assets\img\projects\lv_segmentation\table2.PNG" alt="table1" >
                <h3>Table 2. Comparison of evaluation measures (DC), (HD), and (IoU) for the U-Net model between five possible scenarios</h3>
              </div>

              <div class="col-12 col-lg-5 col-md-5">
                <div class="text-justify">
                  <div align = "justify">
                  <br>
                  <h3>Table 1 provides the average Dice coefficient, Hausdorff distance, and Intersectionover-Union for the three models, across all testing images (199 images).
                    <br>The U-Net model, in comparison with the SegNet and FC-DenseNet models, achieved relatively
                    better performance. The average Hausdorff distance, however, was higher for the FCDenseNet, compared to the other two models.<br><br>
                    For each image, there were four assessments of the LV border; two human and two
                    automated (trained by the annotation of either of human operators). As shown in Table 2,
                    the automated models perform similarly to human operators. <br><br>The automated model
                    disagrees with the Operator-A, but so does the Operator-B. Since different experts make
                    different judgments, it is not possible for any automated model to agree with all experts.
                    However, it is desirable for the automated models do not have larger discrepancies when
                    compared with the performance of human judgments; that is, to behave approximately
                    as well as human operators.
                  </h3>
              </div>
            </div>
            </div>
          </div>
        </div>

        <!-- ======= Project Limitations and conclusion  ======= -->
          <div class = "project Conclusion">
            <div class="container">
              <div class="row justify-content-left">
                <div class="col-12">
                  <div align="justify">
                  <hr>
                  <h2><b>Conclusion and Future Work</b></h2>
                  <h3><br><p class="text-justify">The time-consuming and operator-dependent process of manual annotation of left ventricle border on a 2D echocardiographic recording could be assisted by the automated
                        models that do not require human intervention. Our study investigated the feasibility of
                        such automated models which perform no worse than human experts.<br><br>
                        The automated models demonstrate larger discrepancies with the gold-standard
                        annotations when encountered with the lower image qualities. This is potentially caused
                        by the lack of balanced data in terms of different image quality levels. Since the patient
                        data in our study was obtained by the expert echocardiographers, the distribution leans
                        more towards higher average and higher quality images. This may result in the model
                        forming a bias towards the more condensed quality-level images. Future investigations
                        will examine the correlation between the performance of the deep learning model and
                        the image qualities, as well as using more balanced datasets.<br><br>
                        The patients were a convenience sample drawn from those attending a cardiology
                        outpatient clinic. They, therefore, may not be representative of patients who enter trials
                        with particular enrolment criteria or of inpatients or the general population. A further
                        investigation will look at a wide range of subjects in any cardiovascular disease setting.
                        The segmentation of other cardiac views, and using data acquired by various ultrasound
                        vendors can also be considered for a comprehensive examination of the deep learning
                        models in echocardiography.</h3>

                </div>
                </div>
              </div>
            </div>

                <div class="container">
                  <hr>

                    <div class="row justify-content-left">
                      <div class="col-12">
                        <div class="text-left">
                              <h1> <b> Project Team </b></h1>
                                  <h6>  <a href="https://www.uwl.ac.uk/staff/neda-azarmehr" target="_blank" rel="noopener noreferrer"> Neda Azarmehr </a></h6>
                                <h6> <a href="https://www.uwl.ac.uk/staff/massoud-zolgharni" target="_blank" rel="noopener noreferrer"> Massoud Zolgharni  </a></h6>
                        </div >
                     </div>
                  </div>
                </div>
            <div class="container">
              <!-- test -->
              <hr>

                <div class="row justify-content-left">
                  <div class="col-12">
                    <div class="text-left">
                      <h1> <b>  References</b> </h1>
                      <br>
                      <h3> <a  href="https://link.springer.com/chapter/10.1007/978-3-030-39343-4_43" >Segmentation of Left Ventricle in 2D Echocardiography Using Deep Learning</a></h3>

                    </div>
                  </div>
                </div>

              </div>
          </div>

      <!--    <div class="text-center">
            <a class="btn btn-primary" href="https://link.springer.com/chapter/10.1007/978-3-030-39343-4_43" role="button">Download the paper</a>
          </div>-->




                </section><!-- End project-body -->

            </main><!-- End #main -->


  <!-- ======= Footer ======= -->
  <footer id="footer">


    <div class="footer-top">
      <div class="section-title">
        <h2>Contact Details </h2>

      </div>
      <div class="container">
        <div class="row">

          <div class="col-lg-3 col-md-6 footer-contact">
            <h3><a href="https://www.uwl.ac.uk/research/research-centres-and-groups/intelligent-sensing" target="_blank"
                rel="noopener noreferrer">IntSaV Research Group</a></h3>
            <p>
              C/O Professor Massoud Zolgharni <br>
              University of West London<br>
              St Mary's Rd <br>
              Ealing,<br>
              London, England.<br>
              W5 5RF<br><br>
              <strong>Phone:</strong> 0800 036 8888<br>
              <strong>Email:</strong> Massoud.Zolgharni@uwl.ac.uk<br>
            </p>
          </div>

          <div class="col-lg-2 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="bx bx-chevron-right"></i> <a href="https://www.uwl.ac.uk/intsav" target="_blank"
                  rel="noopener noreferrer">UWL homepage</a></li>
            </ul>
          </div>


        </div>
      </div>
    </div>

    <div class="container d-md-flex py-4">

      <div class="me-md-auto text-center text-md-start">
        <div class="copyright">
          &copy; Copyright <strong><span>IntSaV</span></strong>. All Rights Reserved
        </div>

      </div>
      <div class="social-links text-center text-md-right pt-3 pt-md-0">
        <a href="https://twitter.com/intsav_?lang=en-gb" target="_blank" rel="noopener noreferrer" class="twitter"><i
            class="bx bxl-twitter"></i></a>
        <a href="https://www.linkedin.com/company/intelligent-sensing-and-vision-research-group-intsav" target="_blank"
          rel="noopener noreferrer"><i class="bi bi-linkedin"></i></a>
      </div>
    </div>
  </footer><!-- End Footer -->

  <div id="preloader"></div>
  <a href="#title" class="back-to-top d-flex align-items-center justify-content-center"><i
      class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/purecounter/purecounter.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
