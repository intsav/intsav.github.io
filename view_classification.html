<!DOCTYPE html>
<html lang="en">

<!-- ======= Head Section - DON'T CHANGE ======= -->
<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title>View Classification</title>
  <link href="assets/img/logo.png" rel="icon">
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">
  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">
</head>

<!-- ======= Body Section ======= -->
<body>
  <!-- ======= Navigation Bar - DON'T CHANGE ======= -->
  <header id="header" class="fixed-top">
    <div class="container d-flex align-items-center justify-content-between">
      <a href="index.html" class="logo"><img src="assets/img/logo.png" alt="" class="img-fluid"></a>
      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto" href="index.html">Home</a></li>
          <li><a class="nav-link scrollto" href="index.html #services">Projects</a></li>
          <li><a class="nav-link scrollto" href="index.html #team">Team</a></li>
          <li><a class="nav-link scrollto" href="index.html #contact">Contact</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav>
    </div>
  </header><!-- End Navigation Bar -->

    <!-- ======= Title Section ======= -->
    <section id="project-title" class="d-flex align-items-center">
      <div class="container position-relative" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-12 text-left">
              <h1>Neural Architecture Search of Echocardiography View Classifiers</h1>
            <h2><i>Neda Azarmehr <sup>1,2</sup>, Xujiong Ye <sup>2</sup>, James P Howard <sup>3</sup>, Elisabeth S Lane <sup>1</sup>, Robert Labs <sup>1</sup>, Matthew J Shun-shin <sup>3</sup>, Graham D Cole <sup>3</sup>, Luc Bidaut <sup>2</sup>, Darrel P Francis <sup>3</sup>, Massoud Zolgharni <sup>1,3</sup><i></h2>
            <h3><sup>1</sup> School of Computing and Engineering, University of West London, London, United Kingdom<h3>
            <h3><sup>2</sup> School of Computer Science, University of Lincoln, Lincoln, United Kingdom<h3>
            <h3><sup>3</sup> National Heart and Lung Institute, Imperial College, London, United Kingdom<h3>
            </div>
          </div>
        </div>
    </section><!-- End Title -->

    <!-- Main Webpage Section -->
  <main id="project-main">

    <!-- ======= Project Body Section ======= -->
    <section id="project-body">

      <!-- ======= Project Introduction ======= -->
      <div class="project-intro">
        <div class="container">
          <div class="row justify-content-left">
            <div class="col-12">
              <hr>
              <h2>Purpose:</h2>
                <h3>Echocardiography is the most commonly used modality for assessing the heart in clinical practice. In an echocardiographic exam, an ultrasound probe samples the heart from different orientations and positions, thereby creating different viewpoints for assessing the cardiac function. The determination of the probe viewpoint forms an essential step in automatic echocardiographic image analysis.</h3>
              <h2>Approach:</h2>
                <h3>In this study, convolutional neural networks are used for the automated identification of 14 different anatomical echocardiographic views (larger than any previous study) in a dataset of 8,732 videos acquired from 374 patients. Differentiable architecture search approach was utilised to design small neural network architectures for rapid inference while maintaining high accuracy. The impact of the image quality and resolution, size of the training dataset, and number of echocardiographic view classes on the efficacy of the models were also investigated.</h3>
              <h2>Results:</h2>
                <h3>In contrast to the deeper classification architectures, the proposed models had significantly lower number of trainable parameters (up to 99.9\% reduction), achieved comparable classification performance (accuracy 88.4-96.0\%, precision 87.8-95.2\%, recall 87.1-95.1\%) and real-time performance with inference time per image of 3.6-12.6ms.</h3>
              <h2>Conclusion:</h2>
                <h3>Compared with the standard classification neural network architectures, the proposed models are faster and achieve comparable classification performance. They also require less training data. Such models can be used for real-time detection of the standard views.</h3>
              <br>
            </div>
          </div>
            <div class="text-left">
              <a class="btn btn-primary" href="#" role="button">Download the paper</a>
            </div>
          </div>
        </div>

      <!-- ======= Project Dataset ======= -->
      <div class = "project-dataset">
        <div class="container project-container">
          <div class="row justify-content-left">
            <div class="col-12 col-md-12 col-lg-6 col-xl-6">
            <img class="img-fluid" src="assets/img/projects/view_classification/dataset1.png" alt="dataset">
            </div>
            <div class="col-12 col-md-12 col-lg-6 col-xl-6">
              <h2>Dataset</h2>
              <h3>A random sample of 374 echocardiographic examinations of different patients and performed between 2010 and 2020 was extracted from Imperial College Healthcare NHS Trust’s echocardiogram database. The acquisition of the images was performed by experienced echocardiographers and according to standard protocols, using ultrasound equipment from GE and Philips manufacturers.</h3>

              <h3>Ethical approval was obtained from the Health Regulatory Agency (Integrated Research Application System identifier 243023). Only studies with full patient demographic data and without intravenous contrast administration were included. Automated anonymization was performed to remove all patient-identifiable information.</h3>

              <h3>The videos were annotated manually by an expert cardiologist (JPH), categorising each video into one of 14 classes. Videos thought to show no identifiable echocardiographic features, or which depicted more than one view, were excluded. Altogether, this resulted in 9,098 echocardiographic videos. Of these, 8,732 (96.0\%) videos could be classified as one of the 14 views by the human expert. The remaining 366 videos were not classifiable as a single view, either because the view changed during the video loop, or because the images were completely unrecognisable. The cardiologist's annotations of the videos were used as the ground truth for all constituent frames of that video.</h3>

              <h3>DICOM-formatted videos of varying image sizes (480*640, 600*800, and 768*1024 pixels)}  were then split into constituent frames, and three frames were randomly selected from each video to represent arbitrary stages of the heart cycle, resulting in 41,321 images. The dataset was then randomly split into training (24791 images), validation (8265 images), and testing (8265 images) sub-datasets in a 60:20:20 ratio. Each sub-datasets contained frames from separate echo studies to maintain sample independence.</h3>

              <h3>The relative distribution of echo view classes labelled by the expert cardiologist is displayed in figure and indicates an imbalanced dataset, with a ratio of 3\% (Subcostal-IVC view as the least represented class) to 13\% (PSAX-AV view as the dominant view).</h3>
              <br>
              <div class="text-left">
                <a class="btn btn-primary" href="#contact" role="button">Download the dataset</a>
              </div>
            </div>
          </div>
        </div>

        <!-- ======= Project Architecture ======= -->
      <div class = "project-architecture">

          <div class="container">
            <hr>
            <div class="row justify-content-left">
              <div class="col-12">
              <h2>Network Architecture</h2>
            </div>
              <div class="col-12 col-md-12 col-lg-6 col-xl-6">
                <h3><b> DARTS Method: <P>DARTS method consists of two stages: architecture search and architecture evaluation. <p>Given
                the input images, it first embarks on an architecture search to explore for a computation cell (a small unit of convolutional layers) <p>as the building block of the neural network architecture.
<p>
                  The figure to the right provides an overview of the network architecture.<p> The model comprises:<p> <b>(i) CNN unit:</b> for the encoding of spatial information for each frame of an echocardiographic video input<p> <b>(ii) LSTM units:</b> for the decoding of complex temporal information<p> <b>(iii) a regression unit:</b> for the prediction of the frames of interest.
                  <h3><p><b>Spatial feature extraction:</b> First, a CNN unit is used to extract a spatial feature vector from every cardiac frame in the image sequence. A series of state-of-the-art architectures were employed for the CNN unit. These included ResNet50, InceptionV3, DenseNet, and InceptionResNetV2.
                  <p><b>Temporal feature extraction:</b> The CNN unit above is only capable of handling a single image, transforming it from input pixels into an internal matrix or vector representation. LSTM units are therefore used to process the image features extracted from the entire image sequence by the CNN, i.e. interpreting the features across time steps. Stacks of LSTM units (1-layer to 4-layers) were explored, where the output of each LSTM unit not in the final layer is treated as input to a unit in the next.
                  <p><b>Regression unit:</b> Finally, the output of the LSTM unit is regressed to predict the location of ED and ES frames. The model returns a prediction for each frame in the cardiac sequence (timestep).
                  </h3>
                </div>
                <div class="col-12 col-md-12 col-lg-6 col-xl-6">
                  <br>
                  <img class="img-fluid" src="assets\img\projects\view_classification\Arch.png" alt="echocardiogram" width="450px" height="500px">
                </div>
              </div>
            </div>
          </div>

        <!-- ======= Project Implementation ======= -->
        <div class = "project-implementation">
          <div class="container">
            <div class="row justify-content-left">
              <div class="col-12">
                <hr>
                <h2>Implementation</h2>
                <h3>IMPLEMENTATION DETAILS</h3>
                <div class="text-left">
                  <br>
                  <a class="btn btn-primary" href="" role="button">Download the code</a>
                </div>
              </div>
            </div>
          </div>
        </div>

      <!-- ======= Project Evaluation ======= -->
      <div class = "project-evaluation">
        <div class="container align-middle">
          <hr>
          <div class="row">

            <div class="col-12 col-lg-6 col-md-6 text-center">
              <img class="img-fluid" src="" alt="IMAGE PLACEHOLDER">
            </div>

            <div class="col-12 col-lg-6 col-md-6">
              <div class="text-left">
                <h2>Evaluation metrics</h2>
                <h3>Several metrics were employed to evaluate the performance of the investigated models in this study. Overall accuracy was calculated as the number of correctly classified images as a fraction of the total number of images. Macro average precision and recall (average overall views of per-view measures) were also computed. F1 score was calculated as the harmonic mean of the precision and recall. Since this study is a multi-class problem, F1 score was the weighted average, where the weight of each class was the number of samples from that class.</h3>
              </div>
            </div>

          </div>
        </div>
      </div>

      <!-- ======= Project Results ======= -->
      <div class = "project-results">
        <div class="container">
          <hr>

            <div class="row justify-content-left">
              <div class="col-12">
                <div class="text-left">
                  <h2>Results</h2>
                  <p>
                </div>
              </div>
            </div>


            <div class="row justify-content-left">
              <div class="col-12 col-lg-6">
                <div class="text-left">
                  <h3>INFORMATION ABOUT RESULTS</h3>
                </div>
              </div>
              <div class="col-12 col-lg-6 text-center">
                <img class="img-fluid" src="" alt="IMAGE PLACEHOLDER">
              </div>
            </div>
          </div>



          </section><!-- End project-body -->

      </main><!-- End #main -->

  <!-- ======= Footer - DONT CHANGE ======= -->
  <footer id="footer">

    <div class="footer-top">
      <div class="container">
        <div class="row">

          <div class="col-lg-3 col-md-6 footer-contact">
            <h3>IntSav Research Group</h3>
            <p>
              C/O Professor Massoud Zolgharni <br>
              University of West London<br>
              St Mary's Rd <br>
              Ealing,<br>
              London, England.<br>
              W5 5RF<br><br>
              <strong>Phone:</strong> 0800 036 8888<br>
              <strong>Email:</strong> Massoud.Zolgharni@uwl.ac.uk<br>
            </p>
          </div>

          <div class="col-lg-2 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="bx bx-chevron-right"></i> <a href="https://www.uwl.ac.uk/research/research-centres/intelligent-sensing">UWL homepage</a></li>
            </ul>
          </div>


        </div>
      </div>
    </div>

    <div class="container d-md-flex py-4">

      <div class="me-md-auto text-center text-md-start">
        <div class="copyright">
          &copy; Copyright <strong><span>IntSaV</span></strong>. All Rights Reserved
        </div>

      </div>
      <div class="social-links text-center text-md-right pt-3 pt-md-0">
        <a href="#" class="twitter"><i class="bx bxl-twitter"></i></a>
        <a href="#" class="facebook"><i class="bx bxl-facebook"></i></a>
        <a href="#" class="instagram"><i class="bx bxl-instagram"></i></a>
        <a href="#" class="google-plus"><i class="bx bxl-skype"></i></a>
        <a href="#" class="linkedin"><i class="bx bxl-linkedin"></i></a>
      </div>
    </div>
  </footer><!-- End Footer -->

  <div id="preloader"></div>
  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/purecounter/purecounter.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
