<!DOCTYPE html>
<html lang="en">

<!-- ======= Head Section - DON'T CHANGE ======= -->
<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title> Internship_Projects</title>
  <link href="assets/img/logo.png" rel="icon">
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">
  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">
</head>

<!-- ======= Body Section ======= -->
<body>
  <!-- ======= Navigation Bar - DON'T CHANGE ======= -->
  <header id="header" class="fixed-top">
    <div class="container d-flex align-items-center justify-content-between">
      <a href="index.html" class="logo"><img src="assets/img/logo.png" alt="" class="img-fluid"></a>
      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto" href="index.html">Home</a></li>
          <li><a class="nav-link scrollto" href="https://intsav.github.io/index.html#projects">Projects</a></li>
          <li><a class="nav-link scrollto" href="https://intsav.github.io/index.html#team">Team</a></li>
          <li><a class="nav-link scrollto" href="https://intsav.github.io/index.html#footer">Contact</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav>
    </div>
  </header><!-- End Navigation Bar -->

    <!-- ======= Title Section ======= -->
    <section id="project-title" class="d-flex align-items-center">
      <div class="container position-relative" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-12 text-center">
            <h1>Internship Projects</h1>

            </div>
          </div>
        </div>
    </section><!-- End Title -->

    <!-- Main Webpage Section -->
  <main id="project-main">

    <!-- ======= Project Body Section ======= -->
    <section id="project-body">



      <!-- ======= Project 1 ======= -->

      <div class = "project-1">
        <div class="container project-container">
          <div class="row justify-content-left d-flex flex-wrap align-items-center">
            <div class="row justify-content-left">
              <div class="col-12">
                <div align="justify">
                   <h2 style="color:blue;"><b>Project 1: Fruits Classification</b></h2> <br>
              <h3>
                Fruit classification plays an important role in many industrial applications including factories,
                supermarkets and other fields. The importance of fruit classification can also be seen
                among people with dietary requirements to assist them in selecting the correct categories of fruits.<br>

              <ul>
                <li>Choose the different classes (at least seven classes) from the 131 classes of the
                  Fruits 360 dataset, Report the train, validation and test results. <br>
                  Justify your classification results on test dataset by using AUC of ROC curve.
                  Also provide the relevant performance metrics such sensitivity, etc.
                </li>
                <li>Change the selected classes to Pear (different varieties, Abate, Forelle, Kaiser, Monster, Red, Stone,
                   Williams) or Apples (different varieties: Crimson Snow, Golden, Golden-Red, Granny Smith, Pink Lady,
                   Red, Red Delicious), and report the results providing relevant metrics. <br>
                  Justify the difference between the results of this part and the previous.</li>

              </ul>
            </h3>
          </div>
        </div>
      </div>

      <div class="row justify-content-left">
        <div class="col-12 col-md-12 col-lg-6 col-xl-6" >
          <div align="center">
              <br><br>
             <img class="img-fluid" src="assets\img\projects\UG_MSc\proj1.JPG" width = "100% "  height=" 100%"  alt="IMAGE Dataset">

        </div>
      </div>
        <div class="col-12 col-md-12 col-lg-6 col-xl-6">
          <div align="Justify">
            <h3>
              <b>Dataset Specifications:</b> <br><br>
              Filename format: image_index_100.jpg (e.g. 32_100.jpg) or r_image_index_100.jpg
              (e.g. r_32_100.jpg)4 or r2_image_index_100.jpg or r3_image_index_100.jpg.
<br>
The total number of images: 90483.<br>

The number of classes: 131 (fruits and vegetables).<br><br>

Download the dataset from the link below:<br>

<a href="https://www.kaggle.com/datasets/moltean/fruits?datasetId=5857&sortBy=voteCount" target="_blank" rel="noopener noreferrer">Fruits 360 | Kaggle</a>
<br>
<b>Research Papers: </b><br>
Horea Muresan, Mihai Oltean, Fruit recognition from images using deep learning, Acta Univ. Sapientiae, Informatica Vol. 10, Issue 1, pp. 26-42, 2018.


            </h3>


        </div>
        </div>
      </div>

  </div>
    <hr>
</div>
</div>
<!--
      <div class = "project-1">
        <div class="container project-container">
          <div class="row justify-content-left d-flex flex-wrap align-items-center">
            <div class="col-12 col-md-12 col-lg-6 col-xl-6">
            <img class="img-fluid" src="assets/img/projects/UG_MSc/classification.png" alt="TDI example strip">
            </div>
            <div class="col-12 col-md-12 col-lg-6 col-xl-6">
              <h2 style="color:blue;">Project 1: Fruits Classification</h2>
              <h3>In order to understand the contents of an image, we must apply image classification, which is the task of using computer vision and machine learning algorithms to extract meaning from an image. This action could be as simple as assigning a label to what the image contains, or as advanced as interpreting the contents of an image and returning a human-readable sentence..
              </h3>
              <h3 style="color:blue;">Prerequisite skills: </h3>
              <h3>
                <ul>
                  <li>Basic programming knowledge in Python</li>
                  <li>Basic knowledge in Deep Learning</li>
                  <li>Ideal for students interested in Image Processing</li>
                </ul>
              </h3>
              <br>
            </div>
          </div>
        </div> -->



        <!-- ======= Project 2 ======= -->

        <div class = "project-2">
          <div class="container project-container">
            <div class="row justify-content-left d-flex flex-wrap align-items-center">
              <div class="row justify-content-left">
                <div class="col-12">
                  <div align="justify">
                     <h2 style="color:blue;"><b>Project 2: Human Action Detection</b></h2> <br>
                <h3>
                   Classification of human activities is one of the emerging research areas in the field of computer vision.
                   It can be used in several applications including medical informatics, surveillance, human computer interaction, and task monitoring.<br>

                <ul>
                  <li>Choose the different classes (at least seven classes) from the 15 classes of the
                    Human Action Detection dataset, Report the train, validation and test results. <br>
                    Justify your classification results on test dataset by using AUC of ROC curve.
                    Also provide the relevant performance metrics such sensitivity, etc.

                  </li>
                  <li>Change the selected classes (running, dancing, fighting, cycling, hugging, drinking and eating),
                    and report the results providing relevant metrics. <br>
                    Justify the difference between the results of this part and the previous.
                  </li>

                </ul>
              </h3>
            </div>
          </div>
        </div>



        <div class="row justify-content-left">
          <div class="col-12 col-md-12 col-lg-6 col-xl-6" >
            <div align="Justify">
              <h3>
              <br><br>  <b>Dataset Specifications:</b> <br><br>
                The total number of images: 18000.<br>
                Training set size: 15000 images (one action per image).<br>
                Test set size: 3000 images (one action per image).<br>
                The number of classes: 15 (different actions).<br><br>
                Download the dataset from the link below:<br>

            <a href="https://www.kaggle.com/datasets/emirhanai/human-action-detection-artificial-intelligence" target="_blank" rel="noopener noreferrer">Human Action Detection - Artificial Intelligence | Kaggle </a>

            <br><br>
            <b>Research Papers: </b><br>
          Dash, S.C.B., Mishra, S.R., Srujan Raju, K. et al. Human action recognition using a hybrid deep learning heuristic.
          Soft Comput 25, 13079–13092 (2021).
          <br>
              </h3>


            </div>
        </div>
          <div class="col-12 col-md-12 col-lg-6 col-xl-6">
            <div align="center">
                <br>
               <img class="img-fluid" src="assets\img\projects\UG_MSc\proj2.JPG" width = "100% "  height=" 100%"  alt="IMAGE Dataset">

            </div>
          </div>
        </div>

      </div>
        <hr>
      </div>
      </div>

        <!-- ======= Project 3 ======= -->

        <div class = "project-3">
          <div class="container project-container">
            <div class="row justify-content-left d-flex flex-wrap align-items-center">
              <div class="row justify-content-left">
                <div class="col-12">
                  <div align="justify">
                     <h2 style="color:blue;"><b>Project 3: Echocardiography View Classification</b></h2> <br>
                <h3>
                  In echocardiography, many canonical view types are possible,
                  each displaying distinct aspects of the heart's complex anatomy.
                  As part of routine clinical care, when images are taken the sonographer is intentionally
                  capturing a specific view, but the annotation of the view type is not applied to the image or recorded in
                  the electronic record. Thus, from raw data alone it is difficult to focus on a specific anatomical view of
                  interest.

                <ul>
                  <li>Use TMED-1 to make 3 possible view-type labels available: PLAX, PSAX,
                    or other (meaning something else other than PLAX or PSAX).<br>
                  </li>
                  <li>Use TMED-2 to find the following classes: PLAX, PSAX, A2C, A4C, or Other <br>
                    </li>
                    <li>Report the train, validation and test results. Justify your classification results on test dataset by using AUC of ROC curve.
                    Also provide the relevant performance metrics such sensitivity, etc.</li>

                </ul>
              </h3>
            </div>
          </div>
        </div>

        <div class="row justify-content-left">
          <div class="col-12 col-md-12 col-lg-6 col-xl-6" >
            <div align="center">

               <img class="img-fluid" src="assets\img\projects\UG_MSc\Project3.JPG" width = "100% "  height=" 100%"  alt="IMAGE Dataset">

          </div>
        </div>
          <div class="col-12 col-md-12 col-lg-6 col-xl-6">
            <div align="Justify">
              <h3>
              <br><br>  <b>Dataset Specifications:</b>
                The TMED dataset contains transthoracic echocardiogram (TTE) imagery acquired in the course of
                routine care consistent with American Society of Echocardiography (ASE) guidelines,
                all obtained from 2011-2020 at Tufts Medical Center.
<br>TMED-1 :  set: 260 patients
<br>TMED-2 :  set: 577 patients <br><br>

  Download the dataset from the link below:<br>

  <a href="https://tmed.cs.tufts.edu/data_access.html" target="_blank" rel="noopener noreferrer">Data Access | Tufts Medical Echocardiogram Dataset (TMED)</a>
  <br><br>
  <b>Research Papers: </b><br>
  Neda Azarmehr, Xujiong Ye, James P. Howard, Elisabeth S. Lane, Robert Labs, Matthew J. Shun-Shin,
   Graham D. Cole, Luc Bidaut, Darrel P. Francis, Massoud Zolgharni, "Neural architecture search of
   echocardiography view classifiers," J. Med. Imag. 8(3) 034002 (22 June 2021).<br>
<br>  Zhe Huang, Gary Long, Benjamin Wessler, and Michael C. Hughes”A New Semi-supervised Learning Benchmark for Classifying View and Diagnosing Aortic Stenosis from Echocardiograms,”In Proceedings of the 6th Machine Learning for Healthcare (MLHC) conference, 2021.
</h3>


          </div>
          </div>
        </div>

    </div>
      <hr>
  </div>
  </div>


  <!-- ======= Project 4 ======= -->

  <div class = "project-4">
    <div class="container project-container">
      <div class="row justify-content-left d-flex flex-wrap align-items-center">
        <div class="row justify-content-left">
          <div class="col-12">
            <div align="justify">
               <h2 style="color:blue;"><b>Project 4: Forest Cover Segmentation</b></h2> <br>
          <h3>
             Forest segmentation finds its application in Land use/Land cover. e.g Forest cover is the amount of
             land area that is covered by forest. It may be measured as relative or absolute. We can do this by identifying the forest
             regions and develop a functionality to measure the forest cover from the satellite images itself.
          <ul>
            <li>Segment the forest areas in the satellite images, Report the train,
              validation and test results. Also provide the relevant performance metrics.<br>

            </li>


          </ul>
        </h3>
      </div>
    </div>
  </div>



  <div class="row justify-content-left">
    <div class="col-12 col-md-12 col-lg-6 col-xl-6" >
      <div align="Justify">
        <h3>
        <br><br>  <b>Dataset Specifications:</b> <br><br>
        This dataset contains 5108 aerial images of dimensions 256x256
<br>The meta_data.csv file maintains the information about the aerial images and their respective binary mask
images
<br><br>
  Download the dataset from the link below:<br>
      <a href="https://www.kaggle.com/datasets/quadeer15sh/augmented-forest-segmentation" target="_blank" rel="noopener noreferrer">Forest Aerial Images for Segmentation | Kaggle </a>
<br><br>  <b>Research Papers: </b><br><br>
Gritzner, Daniel, and Jörn Ostermann. "SegForestNet: Spatial-Partitioning-Based Aerial Image Segmentation." arXiv preprint arXiv:2302.01585 (2023).
        </h3>


      </div>
  </div>
    <div class="col-12 col-md-12 col-lg-6 col-xl-6">
      <div align="center">
          <br>
         <img class="img-fluid" src="assets\img\projects\UG_MSc\proj4.JPG" width = "100% "  height=" 100%"  alt="IMAGE Dataset">

      </div>
    </div>
  </div>

</div>
  <hr>
</div>
</div>


<!-- ======= Project 5======= -->

<div class = "project-5">
  <div class="container project-container">
    <div class="row justify-content-left d-flex flex-wrap align-items-center">
      <div class="row justify-content-left">
        <div class="col-12">
          <div align="justify">
             <h2 style="color:blue;"><b>Project 5: Full Body Segmentation</b></h2> <br>
        <h3>
          Human segmentation often use the outcome of person detection in the video.
          Segmentation and tracking of the person in the video have significant applications in monitoring and
          estimating human pose in 2D images and 3D space. The Martial Arts, Dancing and Sports (MADS) dataset,
          which consists of martial arts actions (Tai-chi and Karate), dancing actions (hip-hop and jazz),
           and sports actions (basketball, volleyball, football, rugby, tennis and badminton).

        <ul>
          <li>Segment the person in each image, Report the train,
             validation and test results. Also provide the relevant performance metrics..<br>
          </li>


        </ul>
      </h3>
    </div>
  </div>
</div>

<div class="row justify-content-left">
  <div class="col-12 col-md-12 col-lg-6 col-xl-6" >
    <div align="center">
        <br><br>
       <img class="img-fluid" src="assets\img\projects\UG_MSc\proj5.JPG" width = "100% "  height=" 100%"  alt="IMAGE Dataset">

  </div>
</div>
  <div class="col-12 col-md-12 col-lg-6 col-xl-6">
    <div align="Justify">
      <h3>
        <br> <br> <b>Dataset Specifications:</b> <br><br>
        A total of 1192 images and masks.

<br><br>Download the dataset from the link below:<br>

<a href="https://www.kaggle.com/datasets/tapakah68/segmentation-full-body-mads-dataset" target="_blank" rel="noopener noreferrer">Segmentation Full Body MADS Dataset | Kaggle</a>
<br><br>
<b>Research Papers: </b><br>
Le, V.-H.; Scherer, R. Human Segmentation and Tracking Survey on Masks for MADS Dataset. Sensors 2021, 21, 8397.
</h3>


  </div>
  </div>
</div>

</div>
<hr>
</div>
</div>



<!-- ======= Project 6 ======= -->

<div class = "project-6">
  <div class="container project-container">
    <div class="row justify-content-left d-flex flex-wrap align-items-center">
      <div class="row justify-content-left">
        <div class="col-12">
          <div align="justify">
             <h2 style="color:blue;"><b>Project 6: Right/ Left ventricular Segmentation in Echocardiography dataset</b></h2> <br>
        <h3>
           The segmentation of Left Ventricle (LV) is currently carried out manually by the experts,
           and the automation of this process has proved challenging due to the presence of speckle noise and the
           inherently poor quality of the ultrasound images. The CAMUS dataset, containing 2D apical four-chamber
            and two-chamber view sequences acquired from 500 patients.
           The endocardium and epicardium of the left ventricle and left atrium wall are contoured by experts.
        <ul>
          <li>Segment the left ventricle endocardium (LVEndo), the myocardium (epicardium contour more specifically, named LVEpi) and the left atrium (LA), Report the train,
            validation and test results. Also provide the relevant performance metrics.<br>

          </li>


        </ul>
      </h3>
    </div>
  </div>
</div>

<div class="row justify-content-left">
  <div class="col-12 col-md-12 col-lg-6 col-xl-6" >
    <div align="Justify">
      <h3>
      <br><br>  <b>Dataset Specifications:</b> <br><br>
      The overall CAMUS dataset consists of clinical exams from 500 patients and comprises : i) a training set of 450 patients along with the corresponding manual references based on the analysis of one clinical expert; ii) a testing set composed of 50 new patients.
      The raw input images are provided through the raw/mhd file format.
<br><br>
Download the dataset from the link below:<br>
    <a href="https://humanheart-project.creatis.insa-lyon.fr/database/" target="_blank" rel="noopener noreferrer">Human Heart Project (insa-lyon.fr) </a>
<br><br>  <b>Research Papers: </b><br>
Azarmehr, N., Ye, X., Sacchi, S., Howard, J.P., Francis, D.P., Zolgharni, M. (2020). Segmentation of Left Ventricle in 2D Echocardiography Using Deep Learning, Medical Image Understanding and Analysis. MIUA 2019.
<br>S. Leclerc et al., "Deep Learning for Segmentation Using an Open Large-Scale Dataset in 2D Echocardiography," in IEEE Transactions on Medical Imaging, vol. 38, no. 9, pp. 2198-2210, Sept. 2019, doi: 10.1109/TMI.2019.2900516

      </h3>


    </div>
</div>
  <div class="col-12 col-md-12 col-lg-6 col-xl-6">
    <div align="center">
        <br><br><br>
       <img class="img-fluid" src="assets\img\projects\UG_MSc\proj6.JPG" width = "100% "  height=" 100%"  alt="IMAGE Dataset">

    </div>
  </div>
</div>

</div>
<hr>
</div>
</div>



<!-- ======= Project 7 ======= -->

<div class = "project-7">
  <div class="container project-container">
    <div class="row justify-content-left d-flex flex-wrap align-items-center">
      <div class="row justify-content-left">
        <div class="col-12">
          <div align="justify">
             <h2 style="color:blue;"><b>Project 7: Product Checkout Dataset Object Detection</b></h2> <br>
        <h3>
          Over recent years, emerging interest has occurred in integrating computer vision technology into the
          retail industry. Automatic checkout (ACO) is one of the critical problems in this area which aims to
          automatically generate the shopping list from the images of the products to purchase. A Large-Scale Retail
          Product Checkout Dataset(RPC) is the largest one, containing a significant number of product images across various categories.

        <ul>
          <li>Detect object in the test dataset as checkouts. Report the train,
            validation and test results. Also provide the relevant performance metrics.<br>
          </li>


        </ul>
      </h3>
    </div>
  </div>
</div>

<div class="row justify-content-left">
  <div class="col-12 col-md-12 col-lg-6 col-xl-6" >
    <div align="center">
        <br>
       <img class="img-fluid" src="assets\img\projects\UG_MSc\proj7.JPG" width = "100% "  height=" 100%"  alt="IMAGE Dataset">

  </div>
</div>
  <div class="col-12 col-md-12 col-lg-6 col-xl-6">
    <div align="Justify">
      <h3>
        <br> <br> <b>Dataset Specifications:</b> <br><br>
      This dataset contains 200 product categories and 83,739 images. It includes single-product
      images taken in controlled environment and multi-product checkout images taken at the checkout counter.<br>
       Various annotations are provided for both single-product images and checkout images.
<br><br>Download the dataset from the link below:<br>

<a href="https://www.kaggle.com/datasets/diyer22/retail-product-checkout-dataset" target="_blank" rel="noopener noreferrer">Detector Working | Kaggle</a>
<br><br>
<b>Research Papers: </b><br><br>
Wei, XS., Cui, Q., Yang, L. et al. RPC: a large-scale and fine-grained retail product checkout dataset. Sci. China Inf. Sci. 65, 197101 (2022).</h3>


  </div>
  </div>
</div>

</div>
<hr>
</div>
</div>


<!-- ======= Project 8 ======= -->

<div class = "project-6">
  <div class="container project-container">
    <div class="row justify-content-left d-flex flex-wrap align-items-center">
      <div class="row justify-content-left">
        <div class="col-12">
          <div align="justify">
             <h2 style="color:blue;"><b>Project 8: Fruit Images for Object Detection</b></h2> <br>
        <h3>
          A fruit dataset for object detection 3 different fruits: Apple, Banana, Orange.
        <ul>
          <li>Detect object in the test dataset(Orange, Banana and apple in bounding box). Report the train,
            validation and test results. Also provide the relevant performance metrics.<br>

          </li>


        </ul>
      </h3>
    </div>
  </div>
</div>

<div class="row justify-content-left">
  <div class="col-12 col-md-12 col-lg-6 col-xl-6" >
    <div align="Justify">
      <h3>
      <br>  <b>Dataset Specifications:</b> <br><br>
      240 training images 60 test images.<br>
      3 different types of fruits: Apple, Banana, Orange.<br>
      .xml files in data have coordinates of objects.
<br><br>
Download the dataset from the link below:<br>
    <a href="https://www.kaggle.com/datasets/mbkinaci/fruit-images-for-object-detection" target="_blank" rel="noopener noreferrer">Fruit Images for Object Detection | Kaggle </a>
<br><br>  <b>Research Papers: </b><br>
Sa I, Ge Z, Dayoub F, Upcroft B, Perez T, McCool C. DeepFruits: A Fruit Detection System Using Deep Neural Networks.
Sensors (Basel). 2016 Aug 3;16(8):1222.<br>

Mureșan, Horea & Oltean, Mihai. (2018). Fruit recognition from images using deep learning. Acta Universitatis Sapientiae, Informatica. 10. 26-42. 10.2478/ausi-2018-0002.

      </h3>


    </div>
</div>
  <div class="col-12 col-md-12 col-lg-6 col-xl-6">
    <div align="center">
        <br><br>
       <img class="img-fluid" src="assets\img\projects\UG_MSc\proj8.JPG" width = "100% "  height=" 100%"  alt="IMAGE Dataset">

    </div>
  </div>
</div>

</div>
<hr>
</div>
</div>


<!-- ======= Project 9 ======= -->

<div class = "project-9">
  <div class="container project-container">
    <div class="row justify-content-left d-flex flex-wrap align-items-center">
      <div class="row justify-content-left">
        <div class="col-12">
          <div align="justify">
             <h2 style="color:blue;"><b>Project 9: Early Myocardial Infarction Detection over Multi-view Echocardiography</b></h2> <br>
        <h3>
          Myocardial infarction (MI) is the leading cause of mortality in the world that occurs
          due to a blockage of the coronary arteries feeding the myocardium. An early diagnosis
          of MI and its localization can mitigate the extent of myocardial damage by facilitating early
          therapeutic interventions. HMC-QU dataset is the first publicly shared dataset serving myocardial
          infarction detection on the left ventricle wall. The dataset includes a collection of apical 4-chamber
          (A4C) and apical 2-chamber (A2C) view 2D-echocardiography recordings.
        <ul>
          <li>Propose a method to detect MI over:
              <ul> <li>Single-view echocardiography by using the information extracted from A4C or A2C views. </li>
                  <li>Multi-view echocardiography by merging the information from A4C or A2C views.</li>
                </ul>

          </li> <br>
          <li>Report the train, validation and test results. Also provide the relevant performance metrics.</li>
        </ul>
      </h3>
    </div>
  </div>
</div>

<div class="row justify-content-left">
  <div class="col-12 col-md-12 col-lg-6 col-xl-6" >
    <div align="center">
        <br><br><br><br><br>
       <img class="img-fluid" src="assets\img\projects\UG_MSc\proj9.JPG" width = "100% "  height=" 100%"  alt="IMAGE Dataset">

  </div>
</div>
  <div class="col-12 col-md-12 col-lg-6 col-xl-6">
    <div align="Justify">
      <h3>
        <br> <br> <b>Dataset Specifications:</b> <br><br>
        The dataset includes a collection of apical 4-chamber (A4C) and apical 2-chamber (A2C) view 2D
        echocardiography recordings obtained during the years 2018 and 2019. The echocardiography recordings are acquired via devices from different vendors that are Phillips and GE Vivid (GE-Health-USA) ultrasound machines. The temporal resolution (frame rate per second) of the echocardiography recordings is 25 fps. The spatial resolution varies from 422x636 to 768x1024 pixels.
  The dataset consists of 162 A4C view 2D echocardiography recordings.
  <br>The A4C view recordings belong to 93 MI patients (all first-time and acute MI) and 69 non-MI subjects.
  The dataset consists of 130 A2C view 2D echocardiography recordings that belong to 68 MI patients and 62 non-MI subjects.

<br><br>Download the dataset from the link below:<br>

<a href="https://www.kaggle.com/datasets/aysendegerli/hmcqu-dataset?select=LV+Ground-truth+Segmentation+Masks" target="_blank" rel="noopener noreferrer">HMC-QU Dataset | Kaggle</a>
<br><br>
<b>Research Papers: </b>
Degerli, M. Zabihi, S. Kiranyaz, T. Hamid, R. Mazhar, R. Hamila, and M. Gabbouj, "Early Detection of Myocardial Infarction in Low-Quality Echocardiography," in IEEE Access, vol. 9, pp. 34442-34453, 2021, https://doi.org/10.1109/ACCESS.2021.3059595.
<br><br>S. Kiranyaz, A. Degerli, T. Hamid, R. Mazhar, R. E. F. Ahmed, R. Abouhasera, M. Zabihi, J. Malik, R. Hamila, and M. Gabbouj, "Left Ventricular Wall Motion Estimation by Active Polynomials for Acute Myocardial Infarction Detection," in IEEE Access, vol. 8, pp. 210301-210317, 2020, https://doi.org/10.1109/ACCESS.2020.3038743.


  </div>
  </div>
</div>

</div>
<hr>
</div>
</div>
<!-- ======= Project 8 ======= -->

<div class = "project-6">
  <div class="container project-container">
    <div class="row justify-content-left d-flex flex-wrap align-items-center">
      <div class="row justify-content-left">
        <div class="col-12">
          <div align="justify">
             <h2 style="color:blue;"><b>Project 10: Doppler Mitral inflow echocardiographic_ pixel to velocity</b></h2> <br>
        <h3>
          The project involves working with Doppler Mitral inflow echocardiographic images. The images show measurements of the blood flow velocity.
          Currently,  AI models can detect and measure necessary parameters on Mitral echocardiographic images,  but measurements are acquired in pixel values, while in order to adapt the techniques to real-world scenario,
           automated measurements should be converted to velocity values. <br><br>
          Usually, there is no way of getting the pixel-to-velocity conversion rate apart from the images themself.
          Echocardiographic images contain velocity values, as well as the 0-line, so the project requires student to create a deployable model that will process echocardiographic images and will lead to pixel-to-velocity conversion rate.
          Student is free to use any available computer vision techniques,
          as long as they can be deployed and incorporated into a larger Deep Learning pipeline. <br><br>

          Student will also be able to have some initial consultation with members of IntSav team,  and will work at the AI-lab.
      </h3>
      <div align="center">
          <br><br>
         <img class="img-fluid" src="assets\img\projects\UG_MSc\MI.jpeg"   alt="IMAGE Dataset">

      </div>

    </div>
  </div>
</div>



</div>
<hr>
</div>
</div>

    <!--  <div class = "project-2">

          <div class="container">
            <hr>
            <div class="row justify-content-left">
              <div class="col-12">
              <h2 style="color:blue;"ß>Image Segmentation </h2>
            </div>
              <div class="col-12 col-md-12 col-lg-6 col-xl-6">
                  <h3>Image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as image objects). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics..</h3>
                <h3 style="color:blue;">Prerequisite skills: </h3>
              <h3>
                <ul>
                  <li>Basic programming knowledge in Python</li>
                  <li>Basic knowledge in Deep Learning</li>
                  <li>Ideal for students interested in Image Processing</li>
                </ul>
              </h3>
                </div>
                <div class="col-12 col-md-12 col-lg-6 col-xl-6">
                  <br>
                  <img class="img-fluid" src="assets/img/projects/UG_MSc/segmentation.jpg" alt="IMAGE PLACEHOLDER">
                </div>
              </div>
            </div>
        </div> -->




      <!-- ======= Project 3 ======= -->
      <!--
      <div class = "project-3">
        <div class="container project-container">
          <div class="row justify-content-left d-flex flex-wrap align-items-center">
            <div class="col-12 col-md-12 col-lg-6 col-xl-6">
            <img class="img-fluid" src="assets/img/projects/UG_MSc/TL.png" alt="TDI example strip">
            </div>
            <div class="col-12 col-md-12 col-lg-6 col-xl-6">
              <h2 style="color:blue;">Transfer Learning</h2>
              <h3>Transfer learning is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks. In this project, you will work on classifying images of cats and dogs by using transfer learning from a pre-trained network.
              </h3>
              <h3 style="color:blue;">Prerequisite skills: </h3>
              <h3>
                <ul>
                  <li>Basic programming knowledge in Python</li>
                  <li>Basic knowledge in Deep Learning</li>
                  <li>Ideal for students interested in Image Processing</li>
                </ul>
              </h3>
              <br>
            </div>
          </div>
        </div>
      </div> -->


          </section><!-- End project-body -->

      </main><!-- End #main -->

  <!-- ======= Footer - DONT CHANGE ======= -->
  <footer id="footer">

    <div class="footer-top">
      <div class="container">
        <div class="row">

          <div class="col-lg-3 col-md-6 footer-contact">
            <h3>IntSav Research Group</h3>
            <p>
              C/O Professor Massoud Zolgharni <br>
              University of West London<br>
              St Mary's Rd <br>
              Ealing,<br>
              London, England.<br>
              W5 5RF<br><br>
              <strong>Phone:</strong> 0800 036 8888<br>
              <strong>Email:</strong> Massoud.Zolgharni@uwl.ac.uk<br>
            </p>
          </div>

          <div class="col-lg-2 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="bx bx-chevron-right"></i> <a href="https://www.uwl.ac.uk/research/research-centres/intelligent-sensing">UWL homepage</a></li>
            </ul>
          </div>


        </div>
      </div>
    </div>

    <div class="container d-md-flex py-4">

      <div class="me-md-auto text-center text-md-start">
        <div class="copyright">
          &copy; Copyright <strong><span>IntSaV</span></strong>. All Rights Reserved
        </div>

      </div>
      <div class="social-links text-center text-md-right pt-3 pt-md-0">
        <a href="#" class="twitter"><i class="bx bxl-twitter"></i></a>
        <a href="#" class="facebook"><i class="bx bxl-facebook"></i></a>
        <a href="#" class="instagram"><i class="bx bxl-instagram"></i></a>
        <a href="#" class="google-plus"><i class="bx bxl-skype"></i></a>
        <a href="#" class="linkedin"><i class="bx bxl-linkedin"></i></a>
      </div>
    </div>
  </footer><!-- End Footer -->

  <div id="preloader"></div>
  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/purecounter/purecounter.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
