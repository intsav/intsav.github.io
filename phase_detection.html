<!DOCTYPE html>
<html lang="en">

<!-- ======= Head Section ======= -->
<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title>Phase Detection</title>
  <link href="assets/img/logo.png" rel="icon">
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">
  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <!-- Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">
</head>

<!-- ======= Body Section ======= -->
<body>
  <!-- ======= Navigation Bar ======= -->
  <header id="header" class="fixed-top">
    <div class="container d-flex align-items-center justify-content-between">
      <a href="index.html" class="logo"><img src="assets/img/logo.png" alt="" class="img-fluid"></a>
      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto" href="index.html">Home</a></li>
          <li><a class="nav-link scrollto" href="https://intsav.github.io/index.html#projects">Projects</a></li>
          <li><a class="nav-link scrollto" href="https://intsav.github.io/index.html#team">Team</a></li>
          <li><a class="nav-link scrollto" href="https://intsav.github.io/index.html#footer">Contact</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav>
    </div>
  </header><!-- End Navigation Bar -->

    <!-- ======= Title Section ======= -->
    <section id="project-title" class="d-flex align-items-center">
      <div class="container position-relative" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-12 text-left">
            <h1>Multibeat Echocardiographic Phase Detection Using Deep Neural Networks</h1>
            <h2><i>Elisabeth S Lane <sup>1</sup>, Neda Azarmehr <sup>2</sup>, Jevgeni Jevsikov <sup>1</sup>, James P Howard <sup>2</sup>, Matthew J Shun-shin <sup>2</sup>, Graham D Cole <sup>2</sup>, Darrel P Francis <sup>2</sup>, Massoud Zolgharni <sup>1,2</sup><i></h2>
            <h3><sup>1</sup> School of Computing and Engineering, University of West London, London, United Kingdom<h3>
            <h3><sup>2</sup> National Heart and Lung Institute, Imperial College, London, United Kingdom<h3>
            </div>
          </div>
        </div>
    </section><!-- End Title -->

    <!-- Main Webpage Section -->
  <main id="project-main">

    <!-- ======= Project Body Section ======= -->
    <section id="project-body">

      <!-- ======= Project Introduction ======= -->
      <div class="project-intro">
        <div class="container">
          <div class="row justify-content-left">
            <div class="col-12">
              <hr>
              <h1>Our team has developed an automated model capable of identifying multiple end-systolic and end-diastolic frames in echocardiographic videos of arbitrary length with performance indistinguishable from that of human experts, but with significantly shorter processing time.</h1>
              <br>
            </div>
          </div>
            <div class="text-left">
              <a class="btn btn-primary" href="https://www.sciencedirect.com/science/article/abs/pii/S0010482521001670" role="button">Download the paper</a>
            </div>
          </div>
        </div>

      <!-- ======= Project Dataset ======= -->
      <div class = "project-dataset">
        <div class="container project-container">
          <div class="row justify-content-left">
            <div class="col-12 col-md-12 col-lg-6 col-xl-6">
            <img class="img-fluid" src="assets/img/echo.gif" alt="echocardiogram">
            </div>
            <div class="col-12 col-md-12 col-lg-6 col-xl-6">
              <h2>Dataset</h2>
              <h3>We used 3 datasets in this study: 1 for training & testing, the others for testing only. We have made our patient dataset and models publicly available, thereby providing a benchmark for future studies and allowing for external validation of our approach</h3>
              <h3>Additionally, we used annotations (ground-truth) from several cardiologist experts, allowing for the examination of inter- and intra-observer variability</h3>
              <br>
              <div class="text-left">
                <a class="btn btn-primary" href="#contact" role="button">Download the dataset</a>
              </div>
            </div>
          </div>
        </div>

      <div class="container">
        <div class="row justify-content-left">
          <div class="col-12">
            <h2>A summary of the datasets is as follows:</h2>
            <br>
          </div>
        </div>

          <div class="table-responsive">
            <table class="table table-hover table-dark align-middle">
              <thead>
                <tr>
                  <th scope="col"><h3>Name</h3></th>
                  <th scope="col"><h3>PACS-dataset</h3></th>
                  <th scope="col"><h3>MultiBeat-dataset</h3></th>
                  <th scope="col"><h3>EchoNet-dataset</h3></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <th scope="row">Source</th>
                  <td>Made <b>public</b> for this study<p>NHS Trust PACS Archives, Imperial College Healthcare</td>
                  <td><b>Private</b><p>St Mary’s Hospital</td>
                  <td><b>Public</b><p>Stanford University Hospital<p>echonet.github.io/dynamic</td>
                </tr>
                <tr>
                  <th scope="row">Ultrasound machine</th>
                  <td>Philips Healthcare (iE33 xMATRIX)</td>
                  <td>GE Healthcare (Vivid.i) and Philips Healthcare (iE33 xMATRIX)</td>
                  <td>Siemens Healthineers (Acuson SC2000) and Philips Healthcare (iE33, Epiq 5G, Epiq 7C)</td>
                </tr>
                <tr>
                  <th scope="row">Number of videos/patients</th>
                  <td>1,000</td>
                  <td>40</td>
                  <td>10,030</td>
                </tr>
                <tr>
                  <th scope="row">Length of videos</th>
                  <td>1-3 heartbeats</td>
                  <td>≥ 10 heartbeats</td>
                  <td>1 heartbeat</td>
                </tr>
                <tr>
                  <th scope="row">Ground-truth</th>
                  <td>2 annotations by 2 experts</td>
                  <td>6 annotations by 5 experts (twice by one expert)</td>
                  <td>1 annotation</td>
                </tr>
                <tr>
                  <th scope="row">Original size (pixels)</th>
                  <td>(300-768)×(400-1024)</td>
                  <td>422×636</td>
                  <td>112×112</td>
                </tr>
                <tr>
                  <th scope="row">Frame rate (fps)</th>
                  <td>23-102</td>
                  <td>52-80</td>
                  <td>50</td>
                </tr>
                <tr>
                  <th scope="row">Format</th>
                  <td>DICOM</td>
                  <td>DICOM</td>
                  <td>AVI</td>
                </tr>
                <tr>
                  <th scope="row">Use</th>
                  <td>Training/Testing</td>
                  <td>Testing</td>
                  <td>Testing</td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>

        <!-- ======= Project Architecture ======= -->
      <div class = "project-architecture">

          <div class="container">
            <hr>
            <div class="row justify-content-left">
              <div class="col-12">
              <h2>Network Architecture</h2>
            </div>
              <div class="col-12 col-md-12 col-lg-6 col-xl-6">
                  <h3>Considering the patient image sequences as visual time-series, we adopted Long-term Recurrent Convolutional Networks (CNN+LSTM) for analysing the echocardiographic videos. <p>Such architectures are a class of models that is both spatially and temporally deep, specifically designed for sequence prediction problems (e.g., order of images) with spatial inputs (e.g. 2D structure or pixels in an image).<p>
                    The figure to the right provides an overview of the network architecture.<p> The model comprises:<p> <b>(i) CNN unit:</b> for the encoding of spatial information for each frame of an echocardiographic video input<p> <b>(ii) LSTM units:</b> for the decoding of complex temporal information<p> <b>(iii) a regression unit:</b> for the prediction of the frames of interest.
                    <h3><p><b>Spatial feature extraction:</b> First, a CNN unit is used to extract a spatial feature vector from every cardiac frame in the image sequence. A series of state-of-the-art architectures were employed for the CNN unit. These included ResNet50, InceptionV3, DenseNet, and InceptionResNetV2.
                    <p><b>Temporal feature extraction:</b> The CNN unit above is only capable of handling a single image, transforming it from input pixels into an internal matrix or vector representation. LSTM units are therefore used to process the image features extracted from the entire image sequence by the CNN, i.e. interpreting the features across time steps. Stacks of LSTM units (1-layer to 4-layers) were explored, where the output of each LSTM unit not in the final layer is treated as input to a unit in the next.
                    <p><b>Regression unit:</b> Finally, the output of the LSTM unit is regressed to predict the location of ED and ES frames. The model returns a prediction for each frame in the cardiac sequence (timestep).
                    </h3>
                </div>
                <div class="col-12 col-md-12 col-lg-6 col-xl-6">
                  <br>
                  <img class="img-fluid" src="assets/img/projects/phase_detection/echo_architecture.png" alt="echocardiogram" width="450px" height="500px">
                </div>
              </div>
            </div>
          </div>

        <!-- ======= Project Implementation ======= -->
        <div class = "project-implementation">
          <div class="container">
            <div class="row justify-content-left">
              <div class="col-12">
                <hr>
                <h2>Implementation</h2>
                <h3>The models were implemented using the TensorFlow 2.0 deep learning framework and trained using an NVIDIA GeForce ® GTX 1080 Ti GPU.<p>
                  Random, on the fly augmentation prevented overfitting, such as rotating between -10 and 10 degrees and spatial cropping between 0 and 10 pixels along each axis.<p>
                  Throughout the study, training was conducted over 70 epochs with a batch size of 2 for all models.
                  The PACS-dataset was used to train the models, with a data split of 60%, 20% and 20% for training, validation and testing, respectively.<p>
                  During testing, a sliding window of 30 frames in width with a stride of one was applied, allowing up to 30 predictions of differing temporal importance to be calculated for each timestep. Toward the end of each video, should a segment be fewer than 30 frames in length, it was zero-padded with the added frames removed after completion. Experimentation proved a stack of 2 LSTM layers was the optimum configuration across all models.</h3>
                <div class="text-left">
                  <br>
                  <a class="btn btn-primary" href="https://github.com/intsav/EchoPhaseDetection" role="button">Download the code</a>
                </div>
              </div>
            </div>
          </div>
        </div>

      <!-- ======= Project Evaluation ======= -->
      <div class = "project-evaluation">
        <div class="container align-middle">
          <hr>
          <div class="row">

            <div class="col-12 col-lg-6 col-md-6 text-center">
              <img class="img-fluid" src="assets/img/projects/phase_detection/aaFD.png" alt="aaFD">
            </div>

            <div class="col-12 col-lg-6 col-md-6">
              <div class="text-left">
                <h2>Evaluation metrics</h2>
                <h3>As the primary endpoint for frame detection, evaluation of trained network predictions measures the difference between each labelled target, either ED or ES, and the timestep prediction.<p>
                    Average Absolute Frame Difference (aaFD) notation is applied (to the left), where <i>N</i> is the number of events within the test dataset.<p>
                    The signed mean (μ) and standard deviation (σ) of the error (i.e. frame differences) were also calculated.
                </h3>
              </div>
            </div>

          </div>
        </div>
      </div>

      <!-- ======= Project Results ======= -->
      <div class = "project-results">
        <div class="container">
          <hr>

            <div class="row justify-content-left">
              <div class="col-12">
                <div class="text-left">
                  <h2>Results</h2>
                  <h3><b>PACS-dataset:</b></h3>
                  <p>
                </div>
              </div>
            </div>


            <div class="row justify-content-left">
              <div class="col-12 col-lg-6">
                <div class="text-left">
                  <h3>The average time (mean±SD) taken by the operators to manually annotate ED/ES frames was 26±11 seconds, per event. The equivalent time for our automated models, executed on the GPU, was less than 1.5 seconds; significantly faster than the human-led process.</h3>
                  <h3>Examples of two random patient videos for which the frame detection error is zero, as well as when there is a disagreement between the model’s predictions and expert annotations can be seen to the right. </h3>
                </div>
              </div>
              <div class="col-12 col-lg-6 text-center">
                <img class="img-fluid" src="assets/img/projects/phase_detection/pacs-examples.png" alt="pacs examples">
              </div>
            </div>
          </div>

          <div class="container">
            <div class="row align-middle">
              <div class="col-12 col-lg-7 col-md-7 text-center">
                <img class="img-fluid" src="assets/img/projects/phase_detection/pacs-results.png" alt="pacs results" width="625px">
              </div>

              <div class="col-12 col-lg-5 col-md-5">
                <div class="text-left align-middle">
                  <br>
                  <h3>The table to the left details the error in ED and ES frame detection for all videos in the PACS-dataset. The results indicate the level of disagreement between Operator-1 annotations, considered as the ground-truth, compared with automated predictions and those made by Operator-2.
                  </h3>
              </div>
            </div>
          </div>
        </div>

        <div class="container">
          <div class="row">
            <div class=" col-12 col-lg-8 col-md-8">
              <div class="text-left align-middle">
                <h3>The table to the right provides a comparison between the performance of the model and previously reported deep learning results.
                  The model outperforms almost all existing approaches, indicating smaller discrepancies with the ground-truth from which it has learnt. However, caution is necessary, as different studies have used different private patient datasets, presumably with various levels of image quality and experience of human experts for annotations. Therefore, a direct comparison between the reported accuracies may not be as informative as desired. However, the proposed model’s removal of all pre-processing steps and its capacity to identify multiple heartbeats in one long video is an indisputable advantage.</h3>
                  <h3>For details about the comparable studies, please see the references in our published paper</h3>
              </div>
            </div>
            <div class="col-12 col-lg-4 col-md-4 text-center align-middle">
              <img class="img-fluid" src="assets/img/projects/phase_detection/previously-reported-results.png" alt="pacs reported results" >
            </div>
          </div>
        </div>

              <div class="container">
                <div class="row justify-content-left align-middle">
                  <div class="col-12 col-lg-6 col-md-6">
                    <div class="text-left align-middle">
                      <h3><b>Multibeat-dataset:</b></h3><br>
                        <h3>The Multibeat dataset was annotated by 5 experts, one of whom annotated twice for the evaluation of intra-observer variability. We refer to Operator-1a as the first set of annotations, and Opoerator-1b as the second set of annotations by the same expert</h3>
                        <h3>The table to the right details detection errors between Operator-1 and detections made by the model and other operators. The model disagrees with Operator-1, as do Operators 2-5. Indeed, Operator-1 disagreed with themselves on their second annotation attempt. The smallest error was the discrepancy between the two annotations on separate occasions by the same operator (i.e. intra-observer variability), with a mean difference -0.22±2.76 and 0.25±3.75 for ED and ES events, respectively.</h3>
                      <h3>The range of mean difference between two different operators (i.e. inter-observer variability) was [-0.87, -5.51]±[2.29, 4.26] and [-0.97, -3.46]±[3.67, 4.68] for ED and ES events, respectively. The model discrepancy falls within the range of inter-observer variability. Clearly demonstrating the reliability of the model in frame detection, compared with the experienced human experts.</h3>
                    </div>
                  </div>

                  <div class="col-12 col-lg-6 col-md-6 text-center align-middle">
                    <br><br>
                    <img class="img-fluid" src="assets/img/projects/phase_detection/multibeat-results.png" alt="multibeat img" >
                  </div>
                </div>
            </div>

            <div class="container">
              <div class="row justify-content-left align-middle">
                <div class="col-12">
                  <h3><b>EchoNet-dataset:</b></h3><br>
                  <h3>
                    Previously, the proposed model was compared against alternative reported approaches. However, each study used a different private dataset, making a direct comparison extremely difficult. Here, we applied our model to the publicly available EchoNet-dataset, allowing for future studies to be benchmarked against ours. Like the MultiBeat-dataset, no further training was carried out, and the dataset was used in its entirety for testing.
                    From the total number of videos (10,000), 810 were excluded owing to one of the ED or ES events occurring in the penultimate or final frame in the video, hence being unsuitable.  EchoNet was made available for a challenge focused on segmentation of the left ventricular. Therefore, it was acceptable to have ED or ES events occurring in first or last frames. The retained 9,190 videos were fed into the model, when no resampling of the images was required as the dataset is provided with a resolution of 112×112 pixels; identical to the input size of our model.
                    An aaFD of 2.30 and 3.49 frames was obtained for ES and ES events, respectively and the mean frame difference was 0.16±3.56 and 2.64±3.59 for ED and ES; well within the range of inter-observer variability already observed.
                  </h3>
                  <br>
                  <h3><b><i>If you have any questions about this work, please email Elisabeth.Lane@uwl.ac.uk</b></i></h3>
                </div>
              </div>
            </div>

          </div>
        </div>



    <!-- ======= Contact Section ======= -->
      <section id="contact" class="contact">
        <div class="container">
          <div class="section-title">
            <hr>
            <h2>Request Access to the project dataset</h2>
            <h3>We have made the PACS-dataset used for training and testing our multibeat phase detection model public for the purpose of benchmarking against future studies.</h3>
            <br><h3><b><i>If you would like to request access to the PACS-dataset, please complete the form below.<i><b></h3>
            <h3>Please note that the EchoNet dataset used for testing only in this study is available at the following URL: https://echonet.github.io/dynamic/</h3>
            <br>
            <h2>Important information about the PACS-dataset:</h2>
              <h3>We took a large random sample of echocardiographic studies from different patients performed between 2010 and 2020 from Imperial College Healthcare NHS Trust’s echocardiogram database. Ethical approval was obtained from the Health Regulatory Agency for the anonymised export of large quantities of imaging data. It was not necessary to approach patients individually for consent of data originally acquired for clinical purposes.
                The images were acquired during examinations performed by experienced echocardiographers, according to the standard protocols for using ultrasound equipment from GE and Philips manufacturers. Only studies with full patient demographic data, and without intravenous contrast administration, were included. Automated anonymisation was performed to remove the patient-identifiable information.
                A CNN model, previously developed in our research group to detect different echocardiographic views, was then used to identify and separate the A4C views. A total of 1,000 videos from different patients of varying lengths, were randomly selected.
                Two accredited and experienced cardiology experts manually selected ED and ES frames, each blinded to the judgment of the other. We developed a custom-made program closely replicating the interface of clinical echocardiography hardware. Operators visually inspected the cine loops by controlled animation using a trackball, or arrow keys. The operators were asked to pick ED and ES frames in the A4C view, as they would in preparation for a Biplane Simpson’s measurement in clinical practice. All image sequences were down sampled by cubic interpolation into a standardised size of 112×112 pixels.<h3>
              <br>
              <h3>The image below shows a snapshot of the labels accompanying the dataset which you will receive once your access request has been accepted.</h3>
              <h3><b>Op1</b> refers to operator 1, annotations from whom our network was trained/tested upon.</h3>
              <h3><b>Op2</b> refers to operator 2, the second expert to annotate the dataset. Operator 2 annotations were used for testing and evaluation of observer variability only.</h3>
              <h3><b>AVIname<b> refers to the name of the avi file</h3>
              <h3><b>Number Of Frames<b> is the total number of frames in the video</h3>
              <h3><b>Training/Testing</b> refers to whether the video was used for training or testing only</h3>
              <h3><b>Beats</b> for ease of use, we have limited the number of beats in each video to 4. If you wish to have the unlimited beats label file, please specify when you request access to the dataset</h3>
              <h3>Where there is no annotation in a cell, it means the first frame of interest is ES or the annotator did not select a frame</h3>
</div>
</div>

              <div class="container">
                <div class="row">
                  <div class="col-12">
                    <img class="img-fluid" src="assets/img/projects/phase_detection/labels-example.png" alt="example labels" >
                  </div>
                </div>
              </div>

<div class="container">
  <div class="section-title">
    <h2><b>If you wish to request access, please complete the form below:<b></h2>
      <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">
        <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" />
        <br>
      </a><br /><h3>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</h3></a>
      <br>
      <h3><b> Please consider citing if you make use of the data:</b>
<i>Lane, Elisabeth S., Neda Azarmehr, Jevgeni Jevsikov, James P. Howard, Matthew J. Shun-Shin, Graham D. Cole, Darrel P. Francis, and Massoud Zolgharni. "Multibeat echocardiographic phase detection using deep neural networks." Computers in Biology and Medicine 133 (2021): 104373.</i></h3>
<br>
    </div>
          <div class="row">

            <div class="col-12">

              <form action="https://formspree.io/f/myylgpzr" method="post" role="form" class="php-email-form">
                <div class="row justify-content-center align-middle">
                  <div class="col-6 form-group">
                    <input type="text" name="name" class="form-control" id="name" placeholder="Your Name" required>
                  </div>
                  </div>
                <div class="row justify-content-center align-middle">
                  <div class="col-6 form-group">
                    <input type="email" class="form-control" name="email" id="email" placeholder="Your Email" required>
                  </div>
                </div>
                <div class="row justify-content-center align-middle">
                <div class="col-6 form-group ">
                  <input type="text" class="form-control" name="institution" id="subject" placeholder="Institution/Workplace" required>
                </div>
                </div>
                <div class="row justify-content-center align-middle">
                <div class="col-9 form-group">
                  <textarea class="form-control" name="message" rows="5" placeholder="Please let us know why you would like access to this dataset and what you intend to use it for" required></textarea>
                </div>
                </div>
                <br>
                <div class="text-center"><button type="submit">Submit</button></div>
              </form>

            </div>

          </div>

        </div>
      </section><!-- End Contact Section -->

    </section><!-- End project-body -->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">

    <div class="footer-top">
      <div class="container">
        <div class="row">

          <div class="col-lg-3 col-md-6 footer-contact">
            <h3>IntSav Research Group</h3>
            <p>
              C/O Professor Massoud Zolgharni <br>
              University of West London<br>
              St Mary's Rd <br>
              Ealing,<br>
              London, England.<br>
              W5 5RF<br><br>
              <strong>Phone:</strong> 0800 036 8888<br>
              <strong>Email:</strong> Massoud.Zolgharni@uwl.ac.uk<br>
            </p>
          </div>

          <div class="col-lg-2 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="bx bx-chevron-right"></i> <a href="https://www.uwl.ac.uk/research/research-centres/intelligent-sensing">UWL homepage</a></li>
            </ul>
          </div>


        </div>
      </div>
    </div>

    <div class="container d-md-flex py-4">

      <div class="me-md-auto text-center text-md-start">
        <div class="copyright">
          &copy; Copyright <strong><span>IntSaV</span></strong>. All Rights Reserved
        </div>

      </div>
      <div class="social-links text-center text-md-right pt-3 pt-md-0">
        <a href="#" class="twitter"><i class="bx bxl-twitter"></i></a>
        <a href="#" class="facebook"><i class="bx bxl-facebook"></i></a>
        <a href="#" class="instagram"><i class="bx bxl-instagram"></i></a>
        <a href="#" class="google-plus"><i class="bx bxl-skype"></i></a>
        <a href="#" class="linkedin"><i class="bx bxl-linkedin"></i></a>
      </div>
    </div>
  </footer><!-- End Footer -->

  <div id="preloader"></div>
  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/purecounter/purecounter.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
